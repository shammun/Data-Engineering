{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Udacity Data Engineer Nanodegree - Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Project Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "This project builds upon 4 datasetrs included in the Udacity project workspace. The workflow for this project is described below:\n",
    "\n",
    "1. Scope the project and gather data\n",
    "2. Exploring all the data to understand them, clean them, and possibly save a new copy\n",
    "3. Define the Data Model  based on the exploration \n",
    "4. Design ETL as such and then run it to model the data\n",
    "5. Data quality check\n",
    "6. Other considerations\n",
    "7. Summary and running the project at a glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, dayofweek, hour, weekofyear, date_format, to_date\n",
    "from pyspark.sql.functions import lit, expr\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "AWS_ACCESS_KEY_ID = config.get('AWS', 'AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = config.get('AWS', 'AWS_SECRET_ACCESS_KEY')\n",
    "output_data = config['S3']['DEST_S3_BUCKET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Path to S3 where the parquet files will be written\n",
    "write_to_path = {\"immigration\": \"final_project/immigration.parquet\",\n",
    "                \"state\": \"final_project/state.parquet\",\n",
    "                \"city\": \"final_project/city.parquet\",\n",
    "                \"temperature\": \"final_project/temperature.parquet\",\n",
    "                \"demographic\": \"final_project/demographic.parquet\",\n",
    "                \"visa\": \"final_project/visa.parquet\",\n",
    "                \"countries\": \"final_project/countries.parquet\",\n",
    "                \"mode\": \"final_project/mode.parquet\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read using spark\n",
    "spark = SparkSession.builder\\\n",
    "        .config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\")\\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0,saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "        .enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Then setup the sparkContext object\n",
    "sc = spark.sparkContext\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 1: Scope the project and the related data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Datasets\n",
    "For this project, I will be mainly working wiht the following 4 datasets hosted in the Udacity Workspace. The target is to upload clean and formatted monthly immigration data and other associated files to S3 to give a base for further exploration and analysis of these data at a later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **I94 Immigration Data:** This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. [This](https://travel.trade.gov/research/reports/i94/historical/2016.html) is where the data comes from. There's also a sample file so you can take a look at the data in csv format before reading it all in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **World Temperature Data:** This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **U.S. City Demographic Data:** This data comes from OpenSoft. You can read more about it [here]().\n",
    "* **Airport Code Table:** This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Downloading and uploading to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Downloading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* I94 Immigration Data: \n",
    "The data is in the  folder with the following path: `../../data/18-83510-I94-Data-2016/`. To download this, I ran the following command in the command prompt.\n",
    "`!zip -r data.zip ../../data/18-83510-I94-Data-2016` This created a zip folder named **data.zip** in the working directory. Now, I just downloaded this to my local computer by right clicking on it and clicking on **Download**. However, this saved data was not used as the data in the Udacity directory was directly asccessed for reshaping, cleaning and then for final upload to S3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* World Temperature Data: \n",
    "The data is in the folder with the following path: `../../data2/`. To download this, I ran the following command in the command prompt.\n",
    "`!zip -r data2.zip ../../data2`\n",
    "This created a zip folder named **data2.zip** in the working directory. Now, like before, this data can also be downloaded to local computer by right clicking on this zipped folder and clicking on **Download**. Anyway, for this case, we can directly work with the data hosted in the Udacity working environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* U.S. City Demographic Data: This data is in the working directory as a single csv file named us-cities-demographics.csv and so just right click it and download, if needed.\n",
    "* Airport Code Table: This file named as airport-codes_csv.csv can also be directly downloaded to the local computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Uploading to S3\n",
    "Now, upload all of these data to a S3 bucket so that this can be later accessed through Spark and processed as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 2: Exploring all these data to understand them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2.a. Explore I94_SAS_Labels_Descriptions.SAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### First get the country code and name and then write this as CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Country code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "with open(\"I94_SAS_Labels_Descriptions.SAS\") as file:\n",
    "    auxiliary_data = file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the data on this starts from 10th row and ends on 298th row, I will now get all these data by looping through these lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country = {}\n",
    "for countries in auxiliary_data[9:298]:\n",
    "    line = countries.split(\"=\")\n",
    "    code = line[0].strip()\n",
    "    country_name = line[1].strip().strip(\"'\")\n",
    "    country[code] = country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_pd = pd.DataFrame(list(country.items()), columns = ['code', 'country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>MEXICO Air Sea, and Not Reported (I-94, no lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>236</td>\n",
       "      <td>AFGHANISTAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>316</td>\n",
       "      <td>ALGERIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102</td>\n",
       "      <td>ANDORRA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                                            country\n",
       "0  582  MEXICO Air Sea, and Not Reported (I-94, no lan...\n",
       "1  236                                        AFGHANISTAN\n",
       "2  101                                            ALBANIA\n",
       "3  316                                            ALGERIA\n",
       "4  102                                            ANDORRA"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_pd.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "country_pd.to_csv(\"countries.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_spark = spark.createDataFrame(country_pd) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "country_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"countries\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Secondly, get the city code and name and then write this as CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* City code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city = {}\n",
    "for cities in auxiliary_data[302:962]:\n",
    "    line = cities.split(\"=\")\n",
    "    code = line[0].strip().strip(\"'\")\n",
    "    city_name = line[1].strip().strip(\"'\")\n",
    "    city[code] = city_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_pd = pd.DataFrame(list(city.items()), columns = ['code', 'city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALC</td>\n",
       "      <td>ALCAN, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ANC</td>\n",
       "      <td>ANCHORAGE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BAR</td>\n",
       "      <td>BAKER AAF - BAKER ISLAND, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAC</td>\n",
       "      <td>DALTONS CACHE, AK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PIZ</td>\n",
       "      <td>DEW STATION PT LAY DEW, AK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code                          city\n",
       "0  ALC        ALCAN, AK             \n",
       "1  ANC        ANCHORAGE, AK         \n",
       "2  BAR  BAKER AAF - BAKER ISLAND, AK\n",
       "3  DAC        DALTONS CACHE, AK     \n",
       "4  PIZ    DEW STATION PT LAY DEW, AK"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "city_pd.to_csv(\"cities.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_spark = spark.createDataFrame(city_pd) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"city\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Thirdly, get the mode code and name and then write this as CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Mode code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mode = {}\n",
    "for modes in auxiliary_data[972:976]:\n",
    "    line = modes.split(\"=\")\n",
    "    code = line[0].strip()\n",
    "    mode_name = line[1].strip().strip(\"'\").strip(\";\").strip(\"'\")\n",
    "    mode[code] = mode_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mode_pd = pd.DataFrame(list(mode.items()), columns = ['code', 'mode'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>mode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Air</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Sea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Land</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>Not reported'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code            mode\n",
       "0    1             Air\n",
       "1    2             Sea\n",
       "2    3            Land\n",
       "3    9  Not reported' "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "mode_pd.to_csv(\"mode.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mode_spark = spark.createDataFrame(mode_pd) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "mode_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"mode\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fourthly, get the state code and name and then write this as CSV file.\n",
    "* State code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state = {}\n",
    "for states in auxiliary_data[981:1036]:\n",
    "    line = states.split(\"=\")\n",
    "    code = line[0].strip().strip(\"'\")\n",
    "    state_name = line[1].strip().strip(\"'\")\n",
    "    state[code] = state_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_pd = pd.DataFrame(list(state.items()), columns = ['code', 'state'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>ALABAMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>ARIZONA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>ARKANSAS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code       state\n",
       "0   AL     ALABAMA\n",
       "1   AK      ALASKA\n",
       "2   AZ     ARIZONA\n",
       "3   AR    ARKANSAS\n",
       "4   CA  CALIFORNIA"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "state_pd.to_csv(\"state.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_spark = spark.createDataFrame(state_pd) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "state_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"state\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Fifthly, get the visa code and name and then write this as CSV file.\n",
    "* Visa code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa = {}\n",
    "for visas in auxiliary_data[1046:1049]:\n",
    "    line = visas.split(\"=\")\n",
    "    code = line[0].strip()\n",
    "    visa_name = line[1].strip()\n",
    "    visa[code] = visa_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa_pd = pd.DataFrame(list(visa.items()), columns = ['code', 'visa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "visa_pd.to_csv(\"visa.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>visa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Student</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  code      visa\n",
       "0    1  Business\n",
       "1    2  Pleasure\n",
       "2    3   Student"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visa_pd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa_spark = spark.createDataFrame(visa_pd) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "visa_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"visa\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2.b. Explore I94 Immigration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration = pd.read_sas(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\", \"sas7bdat\", encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>validres</th>\n",
       "      <th>delete_days</th>\n",
       "      <th>delete_mexl</th>\n",
       "      <th>delete_dup</th>\n",
       "      <th>delete_visa</th>\n",
       "      <th>delete_recdup</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1957.0</td>\n",
       "      <td>10032016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.493846e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>135.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1966.0</td>\n",
       "      <td>10032016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.746006e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.679298e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>213.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.140963e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>245.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20632.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.934535e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1995.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.148758e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>27.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.152545e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>33.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20612.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.150900e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>F2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>687.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20623.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>06172018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.575378e+10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>39.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>694.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20611.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>04162017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.142101e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>41.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20623.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160103</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KE</td>\n",
       "      <td>8.432352e+10</td>\n",
       "      <td>00023</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>42.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>SFR</td>\n",
       "      <td>20623.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160131</td>\n",
       "      <td>SEO</td>\n",
       "      <td>STU</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KE</td>\n",
       "      <td>8.694424e+10</td>\n",
       "      <td>00023</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>44.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20617.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20679.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160331</td>\n",
       "      <td>BCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>1973.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KL</td>\n",
       "      <td>9.238009e+10</td>\n",
       "      <td>661</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>45.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>127.0</td>\n",
       "      <td>HOU</td>\n",
       "      <td>20617.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20679.0</td>\n",
       "      <td>69.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160331</td>\n",
       "      <td>BCH</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>O</td>\n",
       "      <td>U</td>\n",
       "      <td>M</td>\n",
       "      <td>1947.0</td>\n",
       "      <td>12112016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KL</td>\n",
       "      <td>9.237991e+10</td>\n",
       "      <td>661</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>52.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20606.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20608.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20160601</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>N</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>11302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.790577e+10</td>\n",
       "      <td>00147</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0     4.0  2016.0     6.0   135.0   135.0     XXX  20612.0      NaN     NaN   \n",
       "1     5.0  2016.0     6.0   135.0   135.0     XXX  20612.0      NaN     NaN   \n",
       "2     6.0  2016.0     6.0   213.0   213.0     XXX  20609.0      NaN     NaN   \n",
       "3     7.0  2016.0     6.0   213.0   213.0     XXX  20611.0      NaN     NaN   \n",
       "4    16.0  2016.0     6.0   245.0   245.0     XXX  20632.0      NaN     NaN   \n",
       "5    19.0  2016.0     6.0   254.0   276.0     XXX  20612.0      NaN     NaN   \n",
       "6    27.0  2016.0     6.0   343.0   343.0     XXX  20611.0      NaN     NaN   \n",
       "7    33.0  2016.0     6.0   582.0   582.0     XXX  20612.0      NaN     NaN   \n",
       "8    38.0  2016.0     6.0   687.0   687.0     XXX  20623.0      NaN     NaN   \n",
       "9    39.0  2016.0     6.0   694.0   694.0     XXX  20611.0      NaN     NaN   \n",
       "10   41.0  2016.0     6.0   254.0   276.0     SFR  20623.0      1.0      CA   \n",
       "11   42.0  2016.0     6.0   254.0   276.0     SFR  20623.0      1.0      CA   \n",
       "12   44.0  2016.0     6.0   127.0   127.0     HOU  20617.0      1.0     NaN   \n",
       "13   45.0  2016.0     6.0   127.0   127.0     HOU  20617.0      1.0      TX   \n",
       "14   52.0  2016.0     6.0   101.0   101.0     BOS  20606.0      1.0      MA   \n",
       "\n",
       "    depdate  i94bir  i94visa  count  validres  delete_days  delete_mexl  \\\n",
       "0       NaN    59.0      2.0    1.0       1.0          0.0          0.0   \n",
       "1       NaN    50.0      2.0    1.0       1.0          0.0          0.0   \n",
       "2       NaN    27.0      3.0    1.0       1.0          0.0          0.0   \n",
       "3       NaN    23.0      3.0    1.0       1.0          0.0          0.0   \n",
       "4       NaN    24.0      3.0    1.0       1.0          0.0          0.0   \n",
       "5       NaN    21.0      3.0    1.0       1.0          0.0          0.0   \n",
       "6       NaN    32.0      3.0    1.0       1.0          0.0          0.0   \n",
       "7       NaN    18.0      3.0    1.0       1.0          0.0          0.0   \n",
       "8       NaN    19.0      1.0    1.0       1.0          0.0          0.0   \n",
       "9       NaN    20.0      3.0    1.0       1.0          0.0          0.0   \n",
       "10      NaN    22.0      3.0    1.0       1.0          0.0          0.0   \n",
       "11      NaN    27.0      3.0    1.0       1.0          0.0          0.0   \n",
       "12  20679.0    43.0      2.0    1.0       1.0          0.0          0.0   \n",
       "13  20679.0    69.0      2.0    1.0       1.0          0.0          0.0   \n",
       "14  20608.0    31.0      1.0    1.0       1.0          0.0          0.0   \n",
       "\n",
       "    delete_dup  delete_visa  delete_recdup  dtadfile visapost occup entdepa  \\\n",
       "0          0.0          0.0            0.0       NaN      NaN   NaN       Z   \n",
       "1          0.0          0.0            0.0       NaN      NaN   NaN       Z   \n",
       "2          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "3          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "4          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "5          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "6          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "7          0.0          0.0            0.0       NaN      NaN   NaN       T   \n",
       "8          0.0          0.0            0.0       NaN      NaN   NaN       U   \n",
       "9          0.0          0.0            0.0       NaN      NaN   NaN       U   \n",
       "10         0.0          0.0            0.0  20160103      SEO   NaN       G   \n",
       "11         0.0          0.0            0.0  20160131      SEO   STU       G   \n",
       "12         0.0          0.0            0.0  20160331      BCH   NaN       G   \n",
       "13         0.0          0.0            0.0  20160331      BCH   NaN       G   \n",
       "14         0.0          0.0            0.0  20160601      NaN   NaN       O   \n",
       "\n",
       "   entdepd entdepu matflag  biryear   dtaddto gender insnum airline  \\\n",
       "0      NaN       U     NaN   1957.0  10032016    NaN    NaN     NaN   \n",
       "1      NaN       U     NaN   1966.0  10032016    NaN    NaN     NaN   \n",
       "2      NaN       U     NaN   1989.0       D/S    NaN    NaN     NaN   \n",
       "3      NaN       U     NaN   1993.0       D/S    NaN    NaN     NaN   \n",
       "4      NaN       U     NaN   1992.0       D/S    NaN    NaN     NaN   \n",
       "5      NaN       U     NaN   1995.0       D/S    NaN    NaN     NaN   \n",
       "6      NaN       U     NaN   1984.0       D/S    NaN    NaN     NaN   \n",
       "7      NaN       U     NaN   1998.0       D/S    NaN    NaN     NaN   \n",
       "8      NaN       U     NaN   1997.0  06172018    NaN    NaN     NaN   \n",
       "9      NaN       U     NaN   1996.0  04162017    NaN    NaN     NaN   \n",
       "10     NaN       U     NaN   1994.0       D/S      F    NaN      KE   \n",
       "11     NaN       U     NaN   1989.0       D/S      M    NaN      KE   \n",
       "12       O       U       M   1973.0  09302016      M    NaN      KL   \n",
       "13       O       U       M   1947.0  12112016      F    NaN      KL   \n",
       "14       N     NaN       M   1985.0  11302016    NaN    NaN      AA   \n",
       "\n",
       "          admnum  fltno visatype  \n",
       "0   1.493846e+10    NaN       WT  \n",
       "1   1.746006e+10    NaN       WT  \n",
       "2   1.679298e+09    NaN       F1  \n",
       "3   1.140963e+09    NaN       F1  \n",
       "4   1.934535e+09    NaN       F1  \n",
       "5   1.148758e+09    NaN       F1  \n",
       "6   1.152545e+09    NaN       F1  \n",
       "7   1.150900e+09    NaN       F2  \n",
       "8   3.575378e+10    NaN       E2  \n",
       "9   1.142101e+09    NaN       M1  \n",
       "10  8.432352e+10  00023       F1  \n",
       "11  8.694424e+10  00023       F1  \n",
       "12  9.238009e+10    661       B2  \n",
       "13  9.237991e+10    661       B2  \n",
       "14  9.790577e+10  00147       B1  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = None\n",
    "immigration.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['cicid', 'i94yr', 'i94mon', 'i94cit', 'i94res', 'i94port', 'arrdate',\n",
       "       'i94mode', 'i94addr', 'depdate', 'i94bir', 'i94visa', 'count',\n",
       "       'validres', 'delete_days', 'delete_mexl', 'delete_dup', 'delete_visa',\n",
       "       'delete_recdup', 'dtadfile', 'visapost', 'occup', 'entdepa', 'entdepd',\n",
       "       'entdepu', 'matflag', 'biryear', 'dtaddto', 'gender', 'insnum',\n",
       "       'airline', 'admnum', 'fltno', 'visatype'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a look at all the columns\n",
    "immigration.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>validres</th>\n",
       "      <th>delete_days</th>\n",
       "      <th>delete_mexl</th>\n",
       "      <th>delete_dup</th>\n",
       "      <th>delete_visa</th>\n",
       "      <th>delete_recdup</th>\n",
       "      <th>biryear</th>\n",
       "      <th>admnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.574989e+06</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3.574469e+06</td>\n",
       "      <td>3.574989e+06</td>\n",
       "      <td>3.574989e+06</td>\n",
       "      <td>3.513802e+06</td>\n",
       "      <td>3.287918e+06</td>\n",
       "      <td>3.574350e+06</td>\n",
       "      <td>3.574989e+06</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3574989.0</td>\n",
       "      <td>3.574350e+06</td>\n",
       "      <td>3.574989e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.258526e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.162554e+02</td>\n",
       "      <td>3.148681e+02</td>\n",
       "      <td>2.062085e+04</td>\n",
       "      <td>1.070240e+00</td>\n",
       "      <td>2.063585e+04</td>\n",
       "      <td>4.030389e+01</td>\n",
       "      <td>1.885480e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.975696e+03</td>\n",
       "      <td>6.891172e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.888572e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.090522e+02</td>\n",
       "      <td>2.071219e+02</td>\n",
       "      <td>8.782430e+00</td>\n",
       "      <td>4.343662e-01</td>\n",
       "      <td>1.979676e+01</td>\n",
       "      <td>1.810871e+01</td>\n",
       "      <td>3.806378e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.810871e+01</td>\n",
       "      <td>2.863461e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>2.060600e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.880400e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.900000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.625393e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>2.061300e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.062300e+04</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>6.163465e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.275808e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.450000e+02</td>\n",
       "      <td>2.450000e+02</td>\n",
       "      <td>2.062100e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.063200e+04</td>\n",
       "      <td>4.000000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.976000e+03</td>\n",
       "      <td>6.259342e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.949151e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.250000e+02</td>\n",
       "      <td>5.160000e+02</td>\n",
       "      <td>2.062900e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.064300e+04</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.989000e+03</td>\n",
       "      <td>9.857318e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.432838e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.990000e+02</td>\n",
       "      <td>7.600000e+02</td>\n",
       "      <td>2.063500e+04</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>2.074500e+04</td>\n",
       "      <td>1.160000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.016000e+03</td>\n",
       "      <td>1.000000e+11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cicid      i94yr     i94mon        i94cit        i94res  \\\n",
       "count  3.574989e+06  3574989.0  3574989.0  3.574469e+06  3.574989e+06   \n",
       "mean   3.258526e+06     2016.0        6.0  3.162554e+02  3.148681e+02   \n",
       "std    1.888572e+06        0.0        0.0  2.090522e+02  2.071219e+02   \n",
       "min    4.000000e+00     2016.0        6.0  1.010000e+02  1.010000e+02   \n",
       "25%    1.625393e+06     2016.0        6.0  1.350000e+02  1.350000e+02   \n",
       "50%    3.275808e+06     2016.0        6.0  2.450000e+02  2.450000e+02   \n",
       "75%    4.949151e+06     2016.0        6.0  5.250000e+02  5.160000e+02   \n",
       "max    6.432838e+06     2016.0        6.0  9.990000e+02  7.600000e+02   \n",
       "\n",
       "            arrdate       i94mode       depdate        i94bir       i94visa  \\\n",
       "count  3.574989e+06  3.513802e+06  3.287918e+06  3.574350e+06  3.574989e+06   \n",
       "mean   2.062085e+04  1.070240e+00  2.063585e+04  4.030389e+01  1.885480e+00   \n",
       "std    8.782430e+00  4.343662e-01  1.979676e+01  1.810871e+01  3.806378e-01   \n",
       "min    2.060600e+04  1.000000e+00  1.880400e+04  0.000000e+00  1.000000e+00   \n",
       "25%    2.061300e+04  1.000000e+00  2.062300e+04  2.700000e+01  2.000000e+00   \n",
       "50%    2.062100e+04  1.000000e+00  2.063200e+04  4.000000e+01  2.000000e+00   \n",
       "75%    2.062900e+04  1.000000e+00  2.064300e+04  5.400000e+01  2.000000e+00   \n",
       "max    2.063500e+04  9.000000e+00  2.074500e+04  1.160000e+02  3.000000e+00   \n",
       "\n",
       "           count   validres  delete_days  delete_mexl  delete_dup  \\\n",
       "count  3574989.0  3574989.0    3574989.0    3574989.0   3574989.0   \n",
       "mean         1.0        1.0          0.0          0.0         0.0   \n",
       "std          0.0        0.0          0.0          0.0         0.0   \n",
       "min          1.0        1.0          0.0          0.0         0.0   \n",
       "25%          1.0        1.0          0.0          0.0         0.0   \n",
       "50%          1.0        1.0          0.0          0.0         0.0   \n",
       "75%          1.0        1.0          0.0          0.0         0.0   \n",
       "max          1.0        1.0          0.0          0.0         0.0   \n",
       "\n",
       "       delete_visa  delete_recdup       biryear        admnum  \n",
       "count    3574989.0      3574989.0  3.574350e+06  3.574989e+06  \n",
       "mean           0.0            0.0  1.975696e+03  6.891172e+10  \n",
       "std            0.0            0.0  1.810871e+01  2.863461e+10  \n",
       "min            0.0            0.0  1.900000e+03  0.000000e+00  \n",
       "25%            0.0            0.0  1.962000e+03  6.163465e+10  \n",
       "50%            0.0            0.0  1.976000e+03  6.259342e+10  \n",
       "75%            0.0            0.0  1.989000e+03  9.857318e+10  \n",
       "max            0.0            0.0  2.016000e+03  1.000000e+11  "
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Have a quick look at different statistics of the table\n",
    "immigration.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_spark = spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_jun16_sub.sas7bdat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "|245.0|2016.0|   6.0| 103.0| 103.0|    NYC|20606.0|    1.0|     NY|20613.0|  51.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|20160601|    null| null|      G|      O|   null|      M| 1965.0|08292016|     F|  null|     OS|6.1311562733E10|00087|      WT|             0|\n",
      "|350.0|2016.0|   6.0| 103.0| 103.0|    CHI|20606.0|    1.0|     IL|20615.0|  32.0|    1.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|20160601|    null| null|      G|      O|   null|      M| 1984.0|08292016|     F|  null|     OS|6.1322423333E10|00065|      WB|             1|\n",
      "|366.0|2016.0|   6.0| 103.0| 103.0|    CHI|20606.0|    1.0|     IL|20611.0|  42.0|    1.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|20160601|    null| null|      O|      O|   null|      M| 1974.0|08292016|  null|  null|     OS|6.1309659733E10|00065|      WB|             2|\n",
      "|395.0|2016.0|   6.0| 103.0| 103.0|    CHI|20606.0|    1.0|     TX|20622.0|  19.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|20160601|    null| null|      G|      O|   null|      M| 1997.0|08292016|     M|  null|     OS|6.1320876033E10|00065|      WT|             3|\n",
      "|541.0|2016.0|   6.0| 103.0| 112.0|    PHI|20606.0|    1.0|     FL|20624.0|  42.0|    2.0|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|20160601|    null| null|      G|      O|   null|      M| 1974.0|08292016|     M|  null|     AA|6.1310559933E10|00717|      WT|             4|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration = immigration_spark.distinct().withColumn(\"immigration_id\", monotonically_increasing_id())\n",
    "# Show the 5 top rows\n",
    "fact_immigration.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_reshaped = fact_immigration.withColumn(\"year\", col(\"i94yr\").cast(\"integer\"))\n",
    "immigration_reshaped = fact_immigration.withColumnRenamed(\"i94addr\", \"state_code\")\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"month\", col(\"i94mon\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"city_code\", col(\"i94cit\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"origin_country_code\", col(\"i94res\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumnRenamed(\"i94port\", \"port_code\")\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"data_base_sas\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"arrival_date\", expr(\"date_add(data_base_sas, arrdate)\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"mode_code\", col(\"i94mode\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"departure_date\", expr(\"date_add(data_base_sas, depdate)\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"age\", col(\"i94bir\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"visa_code\", col(\"i94visa\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.withColumn(\"birth_year\", col(\"biryear\").cast(\"integer\"))\n",
    "immigration_reshaped = immigration_reshaped.drop(\"i94yr\", \"i94mon\", \"i94cit\", \"i94res\", \"data_base_sas\", \"arrdate\", \"i94mode\", \"depdate\", \"i94bir\", \"i94visa\", \"biryear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_reshaped = immigration_reshaped.drop(\"dtadfile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "|cicid|port_code|state_code|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|month|city_code|origin_country_code|arrival_date|mode_code|departure_date|age|visa_code|birth_year|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "|245.0|      NYC|        NY|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     F|  null|     OS|6.1311562733E10|00087|      WT|             0|    6|      103|                103|  2016-06-01|        1|    2016-06-08| 51|        2|      1965|\n",
      "|350.0|      CHI|        IL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     F|  null|     OS|6.1322423333E10|00065|      WB|             1|    6|      103|                103|  2016-06-01|        1|    2016-06-10| 32|        1|      1984|\n",
      "|366.0|      CHI|        IL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      O|      O|   null|      M|08292016|  null|  null|     OS|6.1309659733E10|00065|      WB|             2|    6|      103|                103|  2016-06-01|        1|    2016-06-06| 42|        1|      1974|\n",
      "|395.0|      CHI|        TX|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     OS|6.1320876033E10|00065|      WT|             3|    6|      103|                103|  2016-06-01|        1|    2016-06-17| 19|        2|      1997|\n",
      "|541.0|      PHI|        FL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     AA|6.1310559933E10|00717|      WT|             4|    6|      103|                112|  2016-06-01|        1|    2016-06-19| 42|        2|      1974|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the 5 top rows\n",
    "immigration_reshaped.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As I couldn't find any description of the columns validres, delete_days, delete_mexl, delete_dup, delete_visa, delete_recdup, entdepa, entdepd, entdepu, insnum, I will now explore these columns to understand what they stand for and whether they have any significance and I should retain them or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|validres|\n",
      "+--------+\n",
      "|     1.0|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"validres\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|delete_days|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"delete_days\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|delete_mexl|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"delete_mexl\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|delete_dup|\n",
      "+----------+\n",
      "|       0.0|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"delete_dup\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|delete_visa|\n",
      "+-----------+\n",
      "|        0.0|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"delete_visa\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|delete_recdup|\n",
      "+-------------+\n",
      "|          0.0|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"delete_recdup\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|entdepa|\n",
      "+-------+\n",
      "|      K|\n",
      "|      F|\n",
      "|      Q|\n",
      "|   null|\n",
      "|      T|\n",
      "|      B|\n",
      "|      M|\n",
      "|      U|\n",
      "|      O|\n",
      "|      J|\n",
      "|      Z|\n",
      "|      A|\n",
      "|      N|\n",
      "|      R|\n",
      "|      G|\n",
      "|      I|\n",
      "|      P|\n",
      "|      H|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"entdepa\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|entdepd|\n",
      "+-------+\n",
      "|      K|\n",
      "|      Q|\n",
      "|   null|\n",
      "|      M|\n",
      "|      L|\n",
      "|      V|\n",
      "|      O|\n",
      "|      D|\n",
      "|      J|\n",
      "|      N|\n",
      "|      W|\n",
      "|      R|\n",
      "|      I|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"entdepd\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|entdepu|\n",
      "+-------+\n",
      "|   null|\n",
      "|      U|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"entdepu\").distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|insnum|\n",
      "+------+\n",
      "| 33174|\n",
      "|  4821|\n",
      "|  3210|\n",
      "|  2069|\n",
      "| 18947|\n",
      "|  2294|\n",
      "|  3959|\n",
      "|  3606|\n",
      "|  0965|\n",
      "| 06660|\n",
      "|    MM|\n",
      "|  5645|\n",
      "| 39645|\n",
      "|  5149|\n",
      "|  1372|\n",
      "|  1669|\n",
      "|  3650|\n",
      "|  5523|\n",
      "|  2393|\n",
      "| 17835|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_reshaped.select(\"insnum\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the columns **validres**, **delete_days**, **delete_mexl**, **delete_dup**, **delete_visa**, **delete_recdup** have only 1 unique value, I decided to delete these columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: double, port_code: string, state_code: string, count: double, visapost: string, occup: string, entdepa: string, entdepd: string, entdepu: string, matflag: string, dtaddto: string, gender: string, insnum: string, airline: string, admnum: double, fltno: string, visatype: string, immigration_id: bigint, month: int, city_code: int, origin_country_code: int, arrival_date: date, mode_code: int, departure_date: date, age: int, visa_code: int, birth_year: int]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_reshaped.drop(\"validres\", \"delete_days\", \"delete_mexl\", \"delete_dup\", \"delete_visa\", \"delete_recdup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Further as the columns **entdepa**, **entdepd**, **entdepu** and **insnum** are not found in other tables, I decided to delete these columns as well as they have little chance to be included or considered for further analysis based on this table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[cicid: double, port_code: string, state_code: string, count: double, validres: double, delete_days: double, delete_mexl: double, delete_dup: double, delete_visa: double, delete_recdup: double, visapost: string, occup: string, matflag: string, dtaddto: string, gender: string, airline: string, admnum: double, fltno: string, visatype: string, immigration_id: bigint, month: int, city_code: int, origin_country_code: int, arrival_date: date, mode_code: int, departure_date: date, age: int, visa_code: int, birth_year: int]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_reshaped.drop(\"entdepa\", \"entdepd\", \"entdepu\", \"insnum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "|cicid|port_code|state_code|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|month|city_code|origin_country_code|arrival_date|mode_code|departure_date|age|visa_code|birth_year|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "|245.0|      NYC|        NY|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     F|  null|     OS|6.1311562733E10|00087|      WT|             0|    6|      103|                103|  2016-06-01|        1|    2016-06-08| 51|        2|      1965|\n",
      "|350.0|      CHI|        IL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     F|  null|     OS|6.1322423333E10|00065|      WB|             1|    6|      103|                103|  2016-06-01|        1|    2016-06-10| 32|        1|      1984|\n",
      "|366.0|      CHI|        IL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      O|      O|   null|      M|08292016|  null|  null|     OS|6.1309659733E10|00065|      WB|             2|    6|      103|                103|  2016-06-01|        1|    2016-06-06| 42|        1|      1974|\n",
      "|395.0|      CHI|        TX|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     OS|6.1320876033E10|00065|      WT|             3|    6|      103|                103|  2016-06-01|        1|    2016-06-17| 19|        2|      1997|\n",
      "|541.0|      PHI|        FL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     AA|6.1310559933E10|00717|      WT|             4|    6|      103|                112|  2016-06-01|        1|    2016-06-19| 42|        2|      1974|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the 5 top rows\n",
    "immigration_reshaped.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_reshaped.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2.c. Explore world temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "file_loc = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "temp_data = pd.read_csv(file_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Århus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0  1743-11-01               6.068                          1.737  Århus   \n",
       "1  1743-12-01                 NaN                            NaN  Århus   \n",
       "2  1744-01-01                 NaN                            NaN  Århus   \n",
       "3  1744-02-01                 NaN                            NaN  Århus   \n",
       "4  1744-03-01                 NaN                            NaN  Århus   \n",
       "\n",
       "   Country Latitude Longitude  \n",
       "0  Denmark   57.05N    10.33E  \n",
       "1  Denmark   57.05N    10.33E  \n",
       "2  Denmark   57.05N    10.33E  \n",
       "3  Denmark   57.05N    10.33E  \n",
       "4  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Look at the first 5 rows\n",
    "temp_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Subset to USA\n",
    "temp_data = temp_data[temp_data['Country'] == 'United States']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>Abilene</td>\n",
       "      <td>United States</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "47555 1820-01-01               2.101                          3.217  Abilene   \n",
       "47556 1820-02-01               6.926                          2.853  Abilene   \n",
       "47557 1820-03-01              10.767                          2.395  Abilene   \n",
       "47558 1820-04-01              17.989                          2.202  Abilene   \n",
       "47559 1820-05-01              21.809                          2.036  Abilene   \n",
       "\n",
       "             Country Latitude Longitude  year  month  \n",
       "47555  United States   32.95N   100.53W  1820      1  \n",
       "47556  United States   32.95N   100.53W  1820      2  \n",
       "47557  United States   32.95N   100.53W  1820      3  \n",
       "47558  United States   32.95N   100.53W  1820      4  \n",
       "47559  United States   32.95N   100.53W  1820      5  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, extract year and month from the first column dt\n",
    "temp_data['dt'] = pd.to_datetime(temp_data['dt'])\n",
    "temp_data['year'] = temp_data['dt'].apply(lambda t: t.year)\n",
    "temp_data['month'] = temp_data['dt'].apply(lambda t: t.month)\n",
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Changing City and Country column values to match the upper case values stored in cities.csv and state.csv\n",
    "temp_data['City'] = temp_data['City'].str.upper()\n",
    "temp_data['Country'] = temp_data['Country'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47555</th>\n",
       "      <td>1820-01-01</td>\n",
       "      <td>2.101</td>\n",
       "      <td>3.217</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47556</th>\n",
       "      <td>1820-02-01</td>\n",
       "      <td>6.926</td>\n",
       "      <td>2.853</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47557</th>\n",
       "      <td>1820-03-01</td>\n",
       "      <td>10.767</td>\n",
       "      <td>2.395</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47558</th>\n",
       "      <td>1820-04-01</td>\n",
       "      <td>17.989</td>\n",
       "      <td>2.202</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47559</th>\n",
       "      <td>1820-05-01</td>\n",
       "      <td>21.809</td>\n",
       "      <td>2.036</td>\n",
       "      <td>ABILENE</td>\n",
       "      <td>UNITED STATES</td>\n",
       "      <td>32.95N</td>\n",
       "      <td>100.53W</td>\n",
       "      <td>1820</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dt  AverageTemperature  AverageTemperatureUncertainty     City  \\\n",
       "47555 1820-01-01               2.101                          3.217  ABILENE   \n",
       "47556 1820-02-01               6.926                          2.853  ABILENE   \n",
       "47557 1820-03-01              10.767                          2.395  ABILENE   \n",
       "47558 1820-04-01              17.989                          2.202  ABILENE   \n",
       "47559 1820-05-01              21.809                          2.036  ABILENE   \n",
       "\n",
       "             Country Latitude Longitude  year  month  \n",
       "47555  UNITED STATES   32.95N   100.53W  1820      1  \n",
       "47556  UNITED STATES   32.95N   100.53W  1820      2  \n",
       "47557  UNITED STATES   32.95N   100.53W  1820      3  \n",
       "47558  UNITED STATES   32.95N   100.53W  1820      4  \n",
       "47559  UNITED STATES   32.95N   100.53W  1820      5  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "temp_data.to_csv(\"temperature.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['dt', 'AverageTemperature', 'AverageTemperatureUncertainty', 'City',\n",
       "       'Country', 'Latitude', 'Longitude', 'year', 'month'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_spark = spark.createDataFrame(temp_data) # Convert to Spark dataframe\n",
    "temp_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"temperature\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2.d. Explore U.S. City Demographic Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic = pd.read_csv('us-cities-demographics.csv', delimiter=';')\n",
    "demographic.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Changing City and State column values to match the upper case values stored in cities.csv and state.csv\n",
    "demographic['City'] = demographic['City'].str.upper()\n",
    "demographic['State'] = demographic['State'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write the data to the directory\n",
    "demographic.to_csv(\"demographic.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'Median Age', 'Male Population', 'Female Population',\n",
       "       'Total Population', 'Number of Veterans', 'Foreign-born',\n",
       "       'Average Household Size', 'State Code', 'Race', 'Count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demographic.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographic_spark = spark.createDataFrame(demographic) # Convert to Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographic_spark = demographic_spark.withColumnRenamed(\"Median Age\", \"med_age\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"Male Population\", \"male_pop\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"Female Population\", \"female_pop\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"Total Population\", \"total_pop\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"Number of Veterans\", \"no_veterans\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"Average Household Size\", \"average_household_size\")\n",
    "demographic_spark = demographic_spark.withColumnRenamed(\"State Code\", \"state_code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographic_spark = demographic_spark.distinct().withColumn(\"demographic_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+------+--------------+\n",
      "|          City|     State|med_age|male_pop|female_pop|total_pop|no_veterans|Foreign-born|average_household_size|state_code|                Race| Count|demographic_id|\n",
      "+--------------+----------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+------+--------------+\n",
      "|URBAN HONOLULU|    HAWAII|   41.4|176807.0|  175959.0|   352766|    23213.0|    101312.0|                  2.69|        HI|               Asian|240978|             0|\n",
      "|       BOULDER|  COLORADO|   29.0| 56342.0|   51000.0|   107342|     4061.0|     12993.0|                  2.24|        CO|Black or African-...|  1615|             1|\n",
      "|        DOTHAN|   ALABAMA|   38.9| 32172.0|   35364.0|    67536|     6334.0|      1699.0|                  2.59|        AL|  Hispanic or Latino|  1704|             2|\n",
      "|        IRVINE|CALIFORNIA|   34.6|125411.0|  131516.0|   256927|     5532.0|    116366.0|                  2.73|        CA|American Indian a...|   799|             3|\n",
      "|   CENTREVILLE|  VIRGINIA|   36.0| 34749.0|   37033.0|    71782|     3779.0|     27797.0|                  2.97|        VA|Black or African-...|  6724|             4|\n",
      "+--------------+----------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the 5 top rows\n",
    "demographic_spark.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, write data as parquet files to S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demographic_spark.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"demographic\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 3: Define the data model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I used star schema for data modeling. This consideration was made so that it would be helpful for data warehouse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Fact table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **immigration_reshaped**: \n",
    "This has the following columns\n",
    "\n",
    "`cicid, port_code, i94addr, count, validres, delete_days, delete_mexl, delete_dup, delete_visa, delete_recdup, dtadfile, visapost, occup, entdepa, entdepd, entdepu, matflag, dtaddto, gender, insnum, airline, admnum, fltno, visatype, immigration_id, year, month, city_code, origin_country_code, arrival_date, mode_code, departure_date, age, visa_code, birth_year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Dimension table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **countries**: \n",
    "This has the following columns\n",
    "\n",
    "`code, country`\n",
    "\n",
    "* **cities**: \n",
    "This has the following columns\n",
    "\n",
    "`code, city`\n",
    "\n",
    "* **mode**: \n",
    "This has the following columns\n",
    "\n",
    "`code, mode`\n",
    "\n",
    "* **state**: \n",
    "This has the following columns\n",
    "\n",
    "`code, state`\n",
    "\n",
    "* **visa**: \n",
    "This has the following columns\n",
    "\n",
    "`code, state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* **temperature**: \n",
    "This has the following columns\n",
    "\n",
    "`dt, AverageTemperature, AverageTemperatureUncertainty, City, Country, Latitude, Longitude, year, month`\n",
    "\n",
    "* **demographic**: \n",
    "This has the following columns\n",
    "\n",
    "`City, State, med_age, male_pop, female_pop, total_pop, no_veterans, Foreign-born, average_household_size, state_code, Race, Count`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "![](data_model2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 4: Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Then, I uploaded all of these factt tables and dimension tables to S3 bucket. For doing so, I created a dictionary with the path for each of these files to S3 bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "write_to_path = {\"immigration\": \"final_project/immigration.parquet\",\n",
    "                \"state\": \"final_project/state.parquet\",\n",
    "                \"city\": \"final_project/city.parquet\",\n",
    "                \"temperature\": \"final_project/temperature.parquet\",\n",
    "                \"demographic\": \"final_project/demographic.parquet\",\n",
    "                \"visa\": \"final_project/visa.parquet\",\n",
    "                \"countries\": \"final_project/countries.parquet\",\n",
    "                \"mode\": \"final_project/mode.parquet\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "I then write all of these data to the following S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "output_data = \"s3a://aws-logs-608251643021-us-west-2/elasticmapreduce/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For example, to write the immigration data to S3 address as saved in the output_data variable, running the following command will do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#immigration_reshaped.write.mode(\"overwrite\").parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All of the above steps are put in one file in **etl.py**, so that by just running this one file one can upload all the 7 dimension and 1 fact tables to S3 as parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Step 5: Data quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "For doing this, I will check that no table uploaded to the S3 is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Immigration has count : 18113\n"
     ]
    }
   ],
   "source": [
    "# Get row count\n",
    "rows = immig_spark.count()\n",
    "print(f\"Immigration has count : {rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18113"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check state data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State has count : 55\n"
     ]
    }
   ],
   "source": [
    "state_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"state\"]))\n",
    "# Get row count\n",
    "rows = state_spark.count()\n",
    "print(f\"State has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check city data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City has count : 660\n"
     ]
    }
   ],
   "source": [
    "city_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"city\"]))\n",
    "# Get row count\n",
    "rows = city_spark.count()\n",
    "print(f\"City has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature has count : 687289\n"
     ]
    }
   ],
   "source": [
    "temp_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"temperature\"]))\n",
    "# Get row count\n",
    "rows = temp_spark.count()\n",
    "print(f\"temperature has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demographic has count : 2891\n"
     ]
    }
   ],
   "source": [
    "demo_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"demographic\"]))\n",
    "# Get row count\n",
    "rows = demo_spark.count()\n",
    "print(f\"demographic has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check visa data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visa has count : 3\n"
     ]
    }
   ],
   "source": [
    "visa_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"visa\"]))\n",
    "# Get row count\n",
    "rows = visa_spark.count()\n",
    "print(f\"visa has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check countries data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "countries has count : 289\n"
     ]
    }
   ],
   "source": [
    "countries_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"countries\"]))\n",
    "# Get row count\n",
    "rows = countries_spark.count()\n",
    "print(f\"countries has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check mode data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode has count : 4\n"
     ]
    }
   ],
   "source": [
    "mode_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"mode\"]))\n",
    "# Get row count\n",
    "rows = mode_spark.count()\n",
    "print(f\"mode has count : {rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As not a single table downloaded from S3 has 0 count, we can say that our table have been successfully uploaded to S3 as parquet files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The same logic has been implemented in the Validator class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "First call the Validator class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from check_data_quality import DataValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "validator = DataValidator(spark, output_data, write_to_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does immigration table pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table immigration passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_immigration()\n",
    "print(\"The table immigration passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does state table pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table state passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_state()\n",
    "print(\"The table state passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does city table pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table city passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_city()\n",
    "print(\"The table city passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does temperature table pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table temperature passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_temperature()\n",
    "print(\"The table temperature passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does temperature demographic pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table demographic passes the test, True or False???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_demographic()\n",
    "print(\"The table demographic passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does table visa pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table visa passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_visa()\n",
    "print(\"The table visa passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does table countries pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table countries passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_countries()\n",
    "print(\"The table countries passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "* Does table mode pass the check?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The table mode passes the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_mode()\n",
    "print(\"The table mode passes the test, True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Do all the tables pass the test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the tables pass the test, True or Fale???    True\n"
     ]
    }
   ],
   "source": [
    "result = validator.check_all()\n",
    "print(\"All the tables pass the test, True or Fale???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Further check  -- Is the data model used here useful for further work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "We will now check whether different tables can be joined together for immigrants flock more to areas where more people born in other countries live or not. For doing so, I will first group-count foreign-born people by states and then will group-count immigrants by their states. If we see that out of the top 3 states for both group-wise counts, 2 or 3 sates are the same, then we can say that this hypothesis is true. This will also establish the usability of the data model that I have used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"demographic\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.functions import col\n",
    "demo_spark = demo_spark.withColumn(\"Foreign-born\", col(\"Foreign-born\").cast(\"int\"))\n",
    "\n",
    "grouped_df = demo_spark.groupBy(\"State\").agg(sum(\"Foreign-born\").alias(\"total_foreign_born\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Sort the data by the \"total_foreign_born\" column in descending order\n",
    "sorted_df = grouped_df.orderBy(grouped_df[\"total_foreign_born\"].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|     State|total_foreign_born|\n",
      "+----------+------------------+\n",
      "|CALIFORNIA|          37059662|\n",
      "|  NEW YORK|          17186873|\n",
      "|     TEXAS|          14498054|\n",
      "+----------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the 5 top rows\n",
    "sorted_df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "So, more or less these states will also appear when we order the total number of immigrants by state if the hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "First add, full state name to immig_spark as immig_spark has the state code and not full names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))\n",
    "state_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# join the two dataframes on the 'code' column\n",
    "immig_spark = immig_spark.join(state_spark, immig_spark[\"state_code\"] == state_spark[\"code\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "| cicid|port_code|state_code|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|month|city_code|origin_country_code|arrival_date|mode_code|departure_date|age|visa_code|birth_year|code|     state|\n",
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "| 596.0|      ATL|        GA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     LH|6.1306916033E10|00444|      WB|  163208757248|    6|      104|                104|  2016-06-01|        1|    2016-06-11| 67|        1|      1949|  GA|   GEORGIA|\n",
      "| 979.0|      NYC|        NY|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     DL|6.1306436733E10|00043|      WT|  163208757249|    6|      104|                104|  2016-06-01|        1|    2016-06-10| 26|        2|      1990|  NY|  NEW YORK|\n",
      "|1701.0|      SFR|        CA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      O|      O|   null|      M|11302016|  null|  null|     LH| 9.795781893E10|00458|      B2|  163208757250|    6|      107|                107|  2016-06-01|        1|    2016-08-20| 60|        2|      1956|  CA|CALIFORNIA|\n",
      "|1771.0|      NYC|        NJ|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|     KRK| null|      G|      O|   null|      M|11302016|     M|  null|     AB| 9.794322283E10|07248|      B2|  163208757251|    6|      107|                107|  2016-06-01|        1|    2016-06-08| 46|        2|      1970|  NJ|NEW JERSEY|\n",
      "|1863.0|      CHI|        IL|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|     KRK| null|      G|      O|   null|      M|11302016|     M|  null|     LO| 9.793811973E10|00001|      B2|  163208757252|    6|      107|                107|  2016-06-01|        1|    2016-06-13|  5|        2|      2011|  IL|  ILLINOIS|\n",
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_spark.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, do the aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "# group the imported immigration data from S3 by state, now column state and count the number of rows\n",
    "state_count = immig_spark.groupBy(\"state\").agg(count(\"*\").alias(\"total_count\"))\n",
    "\n",
    "# sort the data by total_count in descending order\n",
    "state_count = state_count.orderBy(state_count[\"total_count\"].desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|     state|total_count|\n",
      "+----------+-----------+\n",
      "|CALIFORNIA|     603181|\n",
      "|  NEW YORK|     589603|\n",
      "|   FLORIDA|     584520|\n",
      "+----------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "state_count.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "From here, we can see that the top 2 states are the same for both immigrants and for citizens born in other countries. Thus, our assumption is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "**Another approach to the same problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The above task can also be accomplished by directly adding **Foreign-born** field from demo_spark and adding it to immig_spark by joining on the field state. But before that, like just what has been done before, first we need to get the full state name to the immigration file and then we can just do the joining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))\n",
    "state_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"state\"]))\n",
    "demo_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"demographic\"]))\n",
    "# join the two dataframes on the 'code' column to get the state column with full names\n",
    "immig_spark = immig_spark.join(state_spark, immig_spark[\"state_code\"] == state_spark[\"code\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "| cicid|port_code|state_code|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|month|city_code|origin_country_code|arrival_date|mode_code|departure_date|age|visa_code|birth_year|code|     state|\n",
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "| 596.0|      ATL|        GA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     LH|6.1306916033E10|00444|      WB|  163208757248|    6|      104|                104|  2016-06-01|        1|    2016-06-11| 67|        1|      1949|  GA|   GEORGIA|\n",
      "| 979.0|      NYC|        NY|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     DL|6.1306436733E10|00043|      WT|  163208757249|    6|      104|                104|  2016-06-01|        1|    2016-06-10| 26|        2|      1990|  NY|  NEW YORK|\n",
      "|1701.0|      SFR|        CA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      O|      O|   null|      M|11302016|  null|  null|     LH| 9.795781893E10|00458|      B2|  163208757250|    6|      107|                107|  2016-06-01|        1|    2016-08-20| 60|        2|      1956|  CA|CALIFORNIA|\n",
      "+------+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immig_spark.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the immig_spark and demo_spark  both has column named state, rename the column \"state\" of immig_spark to **state_name**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_spark = immig_spark.withColumnRenamed(\"state\", \"state_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Join everything from the demography table to immig_spark and get a new table\n",
    "new_table = immig_spark.join(demo_spark, immig_spark[\"state_name\"] == demo_spark[\"State\"], \"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+-------------+-------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+-----+--------------+\n",
      "|cicid|port_code|state_code|count|validres|delete_days|delete_mexl|delete_dup|delete_visa|delete_recdup|visapost|occup|entdepa|entdepd|entdepu|matflag| dtaddto|gender|insnum|airline|         admnum|fltno|visatype|immigration_id|month|city_code|origin_country_code|arrival_date|mode_code|departure_date|age|visa_code|birth_year|code|state_name|         City|  State|med_age|male_pop|female_pop|total_pop|no_veterans|Foreign-born|average_household_size|state_code|                Race|Count|demographic_id|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+-------------+-------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+-----+--------------+\n",
      "|596.0|      ATL|        GA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     LH|6.1306916033E10|00444|      WB|  163208757248|    6|      104|                104|  2016-06-01|        1|    2016-06-11| 67|        1|      1949|  GA|   GEORGIA|     SAVANNAH|GEORGIA|   30.3| 69389.0|   76295.0|   145684|     9717.0|     10355.0|                  2.57|        GA|  Hispanic or Latino| 9734|  670014898178|\n",
      "|596.0|      ATL|        GA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     LH|6.1306916033E10|00444|      WB|  163208757248|    6|      104|                104|  2016-06-01|        1|    2016-06-11| 67|        1|      1949|  GA|   GEORGIA|SANDY SPRINGS|GEORGIA|   35.4| 48570.0|   56777.0|   105347|     3015.0|     19837.0|                   2.4|        GA|  Hispanic or Latino|13670| 1597727834119|\n",
      "|596.0|      ATL|        GA|  1.0|     1.0|        0.0|        0.0|       0.0|        0.0|          0.0|    null| null|      G|      O|   null|      M|08292016|     M|  null|     LH|6.1306916033E10|00444|      WB|  163208757248|    6|      104|                104|  2016-06-01|        1|    2016-06-11| 67|        1|      1949|  GA|   GEORGIA|WARNER ROBINS|GEORGIA|   31.4| 35093.0|   38909.0|    74002|     6974.0|      3210.0|                   2.5|        GA|Black or African-...|29307|  515396075522|\n",
      "+-----+---------+----------+-----+--------+-----------+-----------+----------+-----------+-------------+--------+-----+-------+-------+-------+-------+--------+------+------+-------+---------------+-----+--------+--------------+-----+---------+-------------------+------------+---------+--------------+---+---------+----------+----+----------+-------------+-------+-------+--------+----------+---------+-----------+------------+----------------------+----------+--------------------+-----+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_table.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cicid',\n",
       " 'port_code',\n",
       " 'state_code',\n",
       " 'count',\n",
       " 'validres',\n",
       " 'delete_days',\n",
       " 'delete_mexl',\n",
       " 'delete_dup',\n",
       " 'delete_visa',\n",
       " 'delete_recdup',\n",
       " 'visapost',\n",
       " 'occup',\n",
       " 'entdepa',\n",
       " 'entdepd',\n",
       " 'entdepu',\n",
       " 'matflag',\n",
       " 'dtaddto',\n",
       " 'gender',\n",
       " 'insnum',\n",
       " 'airline',\n",
       " 'admnum',\n",
       " 'fltno',\n",
       " 'visatype',\n",
       " 'immigration_id',\n",
       " 'month',\n",
       " 'city_code',\n",
       " 'origin_country_code',\n",
       " 'arrival_date',\n",
       " 'mode_code',\n",
       " 'departure_date',\n",
       " 'age',\n",
       " 'visa_code',\n",
       " 'birth_year',\n",
       " 'code',\n",
       " 'state_name',\n",
       " 'City',\n",
       " 'State',\n",
       " 'med_age',\n",
       " 'male_pop',\n",
       " 'female_pop',\n",
       " 'total_pop',\n",
       " 'no_veterans',\n",
       " 'Foreign-born',\n",
       " 'average_household_size',\n",
       " 'state_code',\n",
       " 'Race',\n",
       " 'Count',\n",
       " 'demographic_id']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# new_table.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, select only the two columns \"State\" and \"Foreign-born\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "diff_table = new_table.select(\"state_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# This is where error is thrown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1407.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:259)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:187)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\t... 88 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-986e9dd0a197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdiff_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-2.4.3-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1407.showString.\n: org.apache.spark.SparkException: Could not execute broadcast in 300 secs. You can increase the timeout for broadcasts via spark.sql.broadcastTimeout or disable broadcast join by setting spark.sql.autoBroadcastJoinThreshold to -1\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:150)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:387)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:117)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:259)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.constructDoConsumeFunction(WholeStageCodegenExec.scala:216)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:187)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceBatches(ColumnarBatchScan.scala:144)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:159)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:96)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:40)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:90)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:544)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:598)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:339)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3383)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset$$anonfun$53.apply(Dataset.scala:3364)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3363)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2544)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2758)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor77.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:220)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:146)\n\t... 88 more\n"
     ]
    }
   ],
   "source": [
    "diff_table.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state_name']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff_table.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, group the data by state and count the number of rows and sum the foreign-born column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "# group the data by state and count the number of rows and sum the foreign-born column\n",
    "state_agg = new_table.groupBy(\"State\").agg(count(\"*\").alias(\"total_count\"), sum(\"Foreign-born\").alias(\"total_foreign_born\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Now, select only 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#state_agg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Validate the tables -- are they formatted in a proper way for joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from check_data_quality import DataValidator\n",
    "validator = DataValidator(spark, output_data, write_to_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immig_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"immigration\"]))\n",
    "state_spark = spark.read.parquet(\"{}{}\".format(output_data, write_to_path[\"state\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "field1 = \"state_code\"\n",
    "field2 = \"code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "result = validator.validate_left_join(immig_spark, state_spark, field1, field2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"The merged table has more than one observation: True or False???    {}\".format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 6. Other considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The data should be updated daily as immigrants or visitors come to the airport everday and thus it needs to be updated on a daily basis. But it is also possible to update the data monthly as the SAS dataset is stored for monthly data in this case. Demographic tables could be updated yearly as iut takes some time for survey to conduct and normally surveys at a large scale are conducted on a yearly basis, if not on a 4 or 5 year basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Situation 1: The data was increased by 100x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "AWS Spark can handle pretty large data. But in case, it crosses its capacity, we can resort to AWS EMR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Situation 2: The data populates a dashboard that must be updated on a daily basis by 7am every day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Using Airflow, we can easily do this with scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Situation 3: The database needed to be accessed by 100+ people"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case, we can use Amazon Redshift, or Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 7. Summary and running the project at a glance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this project, I used immigration data, demography data, temperature data and along with these some other related data saved in SAS file for monthly data. For a single month, I extracted the data from these  sources and then reshaped them so as to make them more easier for future analysis or exploration by data scientists or for business data analysis and visualization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "In this case, I used Star schema where the immigration table is the fact table and there are sevcen more dimension tables, namely: state, city, temperature, demographic, visa, countries and mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "All of these tables were prepared using PySpark and then they were uploaded to S3 bucket as parquet files. To do this, running `python etl.py` will do all the steps and tricks. To work with a  different month's data, one just needs to update the location of the immigration file corresponding to that different month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "To check whether the data is loaded correctly or not, a new class **DataValidator** has been defined in the Python file **check_data_quality.py**. Running the method **check_all()** will give either True or False depending on if all the tables have at least one observations in them in S3 or not. By running different methods corresponding to different tables, one can also check the validity of the table separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
